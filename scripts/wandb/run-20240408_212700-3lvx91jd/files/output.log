/home/yash/Desktop/Biorobotics_Lab/moving_mass_RL-master/scripts/ppo2.py:252: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  obs, act, adv, logp_old = torch.tensor(data['obs'],dtype=torch.float,device='cuda:0'),torch.tensor(data['act'],dtype=torch.float,device='cuda:0'), torch.tensor(data['adv'],dtype=torch.float,device='cuda:0'), torch.tensor(data['logp'],dtype=torch.float,device='cuda:0')
/home/yash/Desktop/Biorobotics_Lab/moving_mass_RL-master/scripts/ppo2.py:272: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  obs, ret = torch.tensor(data['obs'],dtype=torch.float,device='cuda:0'), torch.tensor(data['ret'],dtype=torch.float,device='cuda:0')
0 4.00500548403396 0.072 -14.285 0.0
1 8.123728222482935 0.71 -18.531 0.0
2 0.6344276139677596 0.356 -20.669 0.0
3 5.03587436894264 0.051 -18.391 0.0
4 2.60947556953785 0.265 -17.315 0.0
5 1.2408594252153375 0.026 -15.607 0.0
6 1.5417131635072794 0.546 -14.104 0.0
7 1.2582837083154992 0.018 -10.391 0.0
8 2.4655451396897305 0.102 -11.188 0.0
9 0.920931688491488 0.236 -11.915 0.0
10 0.9015180578260413 0.388 -12.407 0.0
11 2.2725746140925605 0.281 -6.619 0.0
12 0.43729829028910955 0.189 -2.436 0.0
13 2.398670348403822 0.048 -2.626 0.0
14 1.7798223473247834 0.188 -1.013 0.0
15 0.48795684940563244 0.005 -1.597 0.0
16 0.8640330415453183 0.092 -1.458 0.0
17 0.4854712975890334 0.043 -5.925 0.0
18 0.5374182168670293 0.673 -18.15 0.0
19 1.4862933893287746 0.005 -30.244 0.0
20 8.11964675864641 0.058 -61.707 0.0
21 6.937806587349953 0.572 -95.853 0.0
22 16.416795586526185 0.568 -138.812 0.0
23 181.09556174412782 0.63 -176.27 0.0

24 8.469649004500923 0.647 -204.691 0.0
25 106.077573641872 0.547 -202.391 0.0
26 1.9775643694116223 0.375 -197.783 0.0
27 29.486774070388975 0.577 -185.18 0.0
28 8.689620043571768 0.691 -153.999 0.0
29 4.434580296207846 0.672 -103.782 0.0
30 1.3186245855261154 0.615 -78.284 0.0
31 3.6399094209862923 0.163 -52.197 0.0
32 3.0680704957763076 0.256 -23.133 0.0
33 0.994171550278594 0.229 -12.722 0.0
34 3.620064914466079 0.016 -20.435 0.0
35 3.4188026395972275 0.204 -37.029 0.0
36 1.400028419796881 0.72 -52.116 0.0
37 8.109761491453305 0.072 -47.42 0.0
38 5.686848519350082 0.085 -52.442 0.0
39 3.549612335891312 0.28 -43.992 0.0

40 2.9732565491538683 0.182 -14.67 0.0
41 5.748587285121006 0.688 -17.383 0.0
42 12.743624339684725 0.213 -41.471 0.0
43 1.1476985782613092 0.645 -22.085 0.0
44 4.387247661885184 0.715 -9.039 0.0
45 5.770829221747987 0.508 -6.359 0.0
46 1.4159416981288362 0.708 -10.946 0.0
47 4.648988483295379 0.003 -47.001 0.0
48 9.454475203724092 0.673 -27.326 0.0
49 2.69224808846018 0.12 -9.236 0.0
50 5.335208379081147 0.301 -9.517 0.0
51 1.2522046335589674 0.257 -10.313 0.0

52 2.9891317003172873 0.019 -17.896 0.0
53 12.436848609826416 0.047 -23.23 0.0
54 3.1015439287183497 0.232 -9.922 0.0
55 4.358133091233318 0.395 -8.816 0.0
56 5.14767389963503 0.065 -71.188 0.0
57 4.947464296744789 0.171 -26.644 0.0
58 5.4252736454898685 0.675 -63.927 0.0
